{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are implementing Reinforce in DOOM env\n",
    "\n",
    "Remember that this is a complementary notebook-guide to the Medium post that probably brought you here, so the explanation you may be seeking is there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/doom_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports block\n",
    "\n",
    "Here we import the modules used on our training. Numpy is used for generating the possible actions list and Matplotlib for some printing at the end. VizDoom and Torch and TorchVision are our main modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our device usage and path to save the network\n",
    "\n",
    "You have the possibility of using it on Intel MKL-powered PyTorch for CPU (as I did, as we have low-cost with reasonable performance) or on GPU if you have one and want to try it. We also set the random seed and use deterministic-only algorithms on CUDA (if used, for reproducebility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() \n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "#use_cuda = False\n",
    "#device = torch.device('cpu')\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "DoubleTensor = torch.cuda.DoubleTensor if use_cuda else torch.DoubleTensor\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We here set the game assetes and load its configuration files.\n",
    "It will popup a game window, but with no move as we are not yet performing actions on it. We also generate the possible actions list here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doom game assets: game env creation and frame resize functiom\n",
    "game = DoomGame()\n",
    "game.load_config(\"health_gathering.cfg\")\n",
    "game.set_doom_scenario_path(\"health_gathering.wad\")\n",
    "game.set_seed(42)\n",
    "game.init()\n",
    "doom_actions = np.identity(3, dtype=int).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also must create a Frame Stacker class\n",
    "\n",
    "Here is our FrameStacker class. we use it to approach the non-Markov propriety issue on our frame-based env. It stacks the last 4 frames and return it with the input shape of our neural network, preprocessing it on the middle of the way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FrameStacker:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        We can set the memory size here.\n",
    "        Our memory is a deque and, on each stack, it concatenates the frames in memory along the axis 0\n",
    "        We also have a transformer from torch that handles the resizing.\n",
    "        \"\"\"\n",
    "        self.memory_size = 4\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        self.reset()\n",
    "        self.transformer = T.Compose([T.ToPILImage(),\n",
    "                                      T.Resize((84,84)),\n",
    "                                      T.ToTensor()])\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        by feeding the deque with zero-tensors we restart the memory.\n",
    "        \"\"\"\n",
    "        for i in range(4):\n",
    "            self.memory.append(torch.zeros(1, 84, 84).to(device))\n",
    "            \n",
    "    def preprocess_frame(self, frame):\n",
    "        \"\"\"\n",
    "        here we handle the cutting and flowing the frame through the transformer\n",
    "        \"\"\"\n",
    "        frame = frame[80:,:]\n",
    "        frame = self.transformer(frame)\n",
    "        return frame.to(device)/255\n",
    "\n",
    "    def stack(self, frame):\n",
    "        \"\"\"\n",
    "        our stack method preprocesses the state and returns it stacked.\n",
    "        \"\"\"\n",
    "        frame = self.preprocess_frame(frame)\n",
    "        self.memory.append(frame)\n",
    "        return torch.cat(tuple(self.memory), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we define the architecture of our Policy Network.\n",
    "\n",
    "Notice that we use many Tanh activation functions in order to bring more variance to the learning process, and also use BatchNorms to speed up training. You will be able to see it on TensorBoard if you feel like it.\n",
    "\n",
    "![](images/network_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, lr):\n",
    "        \"\"\"\n",
    "        We've put Tanh as activation in order to introduce variance on the learning.\n",
    "        Adam optimizer was the best performant.\n",
    "        I encourage you to try other architectures, optimizers and hyperparameters\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.num_actions = 3\n",
    "        \n",
    "        self.conv_net = nn.Sequential(nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "                   nn.BatchNorm2d(32),\n",
    "                   nn.ELU(True),\n",
    "                   nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "                   nn.BatchNorm2d(64),\n",
    "                   nn.ELU(True),\n",
    "                   nn.Conv2d(64, 128, kernel_size=4, stride=2 ),\n",
    "                   nn.BatchNorm2d(128),\n",
    "                   nn.ReLU(True))\n",
    "        \n",
    "        self.linear = nn.Sequential(nn.Linear(1152, 512),\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.Linear(512, 512),\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.Linear(512, self.num_actions),\n",
    "                                    nn.Tanh(),)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        \n",
    "    def forward(self, state_stack):\n",
    "        \"\"\"\n",
    "        simple feedforward method\n",
    "        \"\"\"\n",
    "        x = self.conv_net(state_stack)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.softmax(self.linear(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = state.float().unsqueeze(0)\n",
    "        probs = self.forward(Variable(state))\n",
    "        \n",
    "        #we've decided to use stochastic action learning in order to introduce variance in the learning\n",
    "        distribution = torch.distributions.categorical.Categorical(probs = probs.detach())\n",
    "        highest_prob_action = distribution.sample()\n",
    "        \n",
    "        \n",
    "        log_prob = torch.log(probs.squeeze(0)[highest_prob_action])\n",
    "        \n",
    "        #it returns the useful values for acting and optimizing\n",
    "        return highest_prob_action, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is our Update Policy function\n",
    "\n",
    "We get the normalized and discounted rewards, use it as labels with the log-softmax score function of the outputs of our network. The stochastic action leraning lets us learn the small probability actions too, which is good for training, as there are more situations for us to expose the model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy_network, rewards, log_probs):\n",
    "    discounted_rewards = []\n",
    "\n",
    "    for t in range(len(rewards)):\n",
    "        Gt = 0\n",
    "        pw = 0\n",
    "        for r in rewards[t:]:\n",
    "            Gt = Gt + GAMMA**pw * r\n",
    "            pw += 1\n",
    "        discounted_rewards.append(Gt)\n",
    "\n",
    "    discounted_rewards = torch.tensor(discounted_rewards, device=device)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std())\n",
    "    \n",
    "    policy_gradient = []\n",
    "    for log_prob, Gt in zip(log_probs, discounted_rewards):\n",
    "        policy_gradient.append(-log_prob * Gt)\n",
    "    \n",
    "    policy_network.optimizer.zero_grad()\n",
    "    policy_gradient_ = torch.stack(policy_gradient[t:]).sum()\n",
    "    policy_gradient_.backward(retain_graph=True)\n",
    "    policy_network.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we set up TensorBoard and some global variables\n",
    "\n",
    "After running this cell, you can seek the model architecture (and later its learning) on TensorBoard, with the command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir runs\n",
    "```\n",
    "\n",
    "If you are running this for the second time you may want to delete your TensorBoard previous records:\n",
    "```\n",
    "rm -r runs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(flush_secs = 40)\n",
    "\n",
    "#here we set the global variables\n",
    "GAMMA = .95\n",
    "EPISODES = 5000\n",
    "learning_rate = 0.02\n",
    "\n",
    "#our net and frame-stacker\n",
    "stacker = FrameStacker()\n",
    "policy_net = PolicyNetwork(lr=learning_rate).to(device)\n",
    "\n",
    "#some lists to write the values, if you want to do some in-notebook plotting.\n",
    "num_steps = []\n",
    "avg_numsteps = []\n",
    "all_rewards = []\n",
    "\n",
    "#we leverage this cell to write our graph to TensorBoard.\n",
    "writer.add_graph(policy_net, stacker.stack(torch.zeros(84, 84)).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We run our train steps now\n",
    "\n",
    "While it runs you can watch its actions being taken on the game windows and see its learning metrics on the TensorBoard window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, total_reward: 476.0, length: 576\n",
      "Episode: 2, total_reward: 444.0, length: 544\n",
      "Episode: 3, total_reward: 540.0, length: 640\n",
      "Episode: 4, total_reward: 284.0, length: 384\n",
      "Episode: 5, total_reward: 284.0, length: 384\n",
      "Episode: 6, total_reward: 412.0, length: 512\n",
      "Episode: 7, total_reward: 476.0, length: 576\n",
      "Episode: 8, total_reward: 700.0, length: 800\n",
      "Episode: 9, total_reward: 1980.0, length: 2080\n",
      "Episode: 10, total_reward: 988.0, length: 1088\n",
      "Episode: 11, total_reward: 892.0, length: 992\n",
      "Episode: 12, total_reward: 604.0, length: 704\n",
      "Episode: 13, total_reward: 284.0, length: 384\n",
      "Episode: 14, total_reward: 764.0, length: 864\n",
      "Episode: 15, total_reward: 764.0, length: 864\n",
      "Episode: 16, total_reward: 572.0, length: 672\n",
      "Episode: 17, total_reward: 508.0, length: 608\n",
      "Episode: 18, total_reward: 284.0, length: 384\n",
      "Episode: 19, total_reward: 1148.0, length: 1248\n",
      "Episode: 20, total_reward: 668.0, length: 768\n",
      "Episode: 21, total_reward: 668.0, length: 768\n",
      "Episode: 22, total_reward: 1180.0, length: 1280\n",
      "Episode: 23, total_reward: 988.0, length: 1088\n",
      "Episode: 24, total_reward: 348.0, length: 448\n",
      "Episode: 25, total_reward: 380.0, length: 480\n",
      "Episode: 26, total_reward: 2100.0, length: 2100\n",
      "Episode: 27, total_reward: 284.0, length: 384\n",
      "Episode: 28, total_reward: 380.0, length: 480\n",
      "Episode: 29, total_reward: 284.0, length: 384\n",
      "Episode: 30, total_reward: 892.0, length: 992\n",
      "Episode: 31, total_reward: 284.0, length: 384\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES):\n",
    "    game.new_episode()\n",
    "    state = game.get_state().screen_buffer\n",
    "    \n",
    "    state = stacker.stack(state)\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        action_idx, log_prob = policy_net.get_action(state)\n",
    "        action = doom_actions[action_idx]\n",
    "        reward = game.make_action(action)\n",
    "        done = game.is_episode_finished()\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            stacker.reset()\n",
    "            break\n",
    "            \n",
    "        new_state = game.get_state().screen_buffer\n",
    "        state = stacker.stack(new_state)\n",
    "        \n",
    "    writer.add_scalar(\"steps\", steps, episode)\n",
    "    \n",
    "    update_policy(policy_net, rewards, log_probs)\n",
    "    num_steps.append(steps)\n",
    "    writer.add_scalar(\"avg_steps\", np.mean(num_steps[-10:]), episode)\n",
    "    \n",
    "    avg_numsteps.append(np.mean(num_steps[-10:]))\n",
    "    all_rewards.append(np.sum(rewards))\n",
    "    print(\"Episode: {}, total_reward: {}, length: {}\".format(episode+1, np.sum(rewards), steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we do some in-notebook plotting if you want it.\n",
    "plt.plot(num_steps)\n",
    "plt.plot(avg_numsteps)\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
